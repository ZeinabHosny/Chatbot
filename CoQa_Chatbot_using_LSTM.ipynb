{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoQa Chatbot using LSTM\n",
    "\n",
    "I used the retrieval chatbot (which uses LSTM Model for vectorization) which is described in the blog: https://omarito.me/building-a-basic-fatwa-chat-bot/ but with some modifications.\n",
    "\n",
    "## Modifications:\n",
    "\n",
    "   1. It uses CoQa dataset instead of the dataset of askfm\n",
    "   2. It prints only the top one answer if the simularity of its question is larger than 0.5\n",
    "   3. if the simlrity of top one questions < .5 it prints the top five questions as suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LSTM Model Training\n",
    "I ran this part on colab and saved the final trained model to be used in tha chatbot. I performed some changes in the dataset cleaning phase to be suitable for english words instead of arabic words, i also removed the numbers and punctuations and i didn't use stemming as in the blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################        Dataset cleaning and tokenization          ####################################\n",
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Embedding, LSTM, RepeatVector\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from six.moves import cPickle\n",
    "from  tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "# Defining the Global variables \n",
    "BATCH_SIZE = 32 # Batch size for GPU\n",
    "NUM_WORDS = 10000 # Vocab length\n",
    "MAX_LEN = 20 # Padding length (# of words)\n",
    "LSTM_EMBED = 8 # Number of LSTM nodes\n",
    "\n",
    "# Reading the dataset\n",
    "train_data = pd.read_csv(\"Rearranged_data.csv\")\n",
    "\n",
    "################################                Cleaning                ####################################\n",
    "# Dropping the answers and the context number\n",
    "train_data.drop(['Answers','Context Number'], inplace=True, axis=1)\n",
    "# Removing the questions that having number of words grater than 20 words\n",
    "train_data = train_data[train_data.Questions.apply(lambda x: len(x.split())) < MAX_LEN]\n",
    "# Cleaning the questions\n",
    "# removing all punc. and numbers\n",
    "train_data.Questions = train_data.Questions.apply(lambda x: (re.sub('[^a-zA-Z]', ' ', x)).strip()) \n",
    "# lowercasing, removing repeated spaces\n",
    "train_data.Questions = train_data.Questions.apply(lambda x: ' '.join(x.lower().split()))\n",
    "train_data = train_data[train_data.Questions.apply(len) > 0]\n",
    "\n",
    "################################                Tokenization                ####################################\n",
    "# Fitting the tokenizer on the questions after cleaning\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, lower=False)\n",
    "tokenizer.fit_on_texts(train_data[\"Questions\"].values)\n",
    "# Save the tokenizer for later use\n",
    "cPickle.dump(tokenizer, open(\"lstm-autoencoder-tokenizer.pickle\", \"wb\"))\n",
    "# Tokenizing the questions\n",
    "train_data = tokenizer.texts_to_sequences(train_data[\"Questions\"].values)\n",
    "# Padding sequences that are shorter than MAX_LEN\n",
    "train_data = pad_sequences(train_data, padding='post', truncating='post', maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'what',\n",
       " 2: 'the',\n",
       " 3: 'did',\n",
       " 4: 'was',\n",
       " 5: 'he',\n",
       " 6: 'who',\n",
       " 7: 'is',\n",
       " 8: 'to',\n",
       " 9: 'how',\n",
       " 10: 'it',\n",
       " 11: 'of',\n",
       " 12: 'in',\n",
       " 13: 'they',\n",
       " 14: 'where',\n",
       " 15: 'a',\n",
       " 16: 'does',\n",
       " 17: 'she',\n",
       " 18: 'when',\n",
       " 19: 'do',\n",
       " 20: 'his',\n",
       " 21: 'for',\n",
       " 22: 'many',\n",
       " 23: 'that',\n",
       " 24: 's',\n",
       " 25: 'were',\n",
       " 26: 'have',\n",
       " 27: 'are',\n",
       " 28: 'with',\n",
       " 29: 'and',\n",
       " 30: 'about',\n",
       " 31: 'there',\n",
       " 32: 'her',\n",
       " 33: 'why',\n",
       " 34: 'on',\n",
       " 35: 'this',\n",
       " 36: 'him',\n",
       " 37: 'name',\n",
       " 38: 'from',\n",
       " 39: 'which',\n",
       " 40: 'at',\n",
       " 41: 'one',\n",
       " 42: 'else',\n",
       " 43: 'be',\n",
       " 44: 'had',\n",
       " 45: 'them',\n",
       " 46: 'long',\n",
       " 47: 'people',\n",
       " 48: 'old',\n",
       " 49: 'first',\n",
       " 50: 'other',\n",
       " 51: 'get',\n",
       " 52: 'go',\n",
       " 53: 'has',\n",
       " 54: 'by',\n",
       " 55: 'as',\n",
       " 56: 'any',\n",
       " 57: 'their',\n",
       " 58: 'say',\n",
       " 59: 'time',\n",
       " 60: 'think',\n",
       " 61: 'like',\n",
       " 62: 'want',\n",
       " 63: 'would',\n",
       " 64: 'kind',\n",
       " 65: 'an',\n",
       " 66: 'been',\n",
       " 67: 'happened',\n",
       " 68: 'after',\n",
       " 69: 'up',\n",
       " 70: 'year',\n",
       " 71: 'will',\n",
       " 72: 'not',\n",
       " 73: 'much',\n",
       " 74: 'take',\n",
       " 75: 'doing',\n",
       " 76: 'or',\n",
       " 77: 'out',\n",
       " 78: 'day',\n",
       " 79: 'can',\n",
       " 80: 'then',\n",
       " 81: 'make',\n",
       " 82: 'anyone',\n",
       " 83: 'called',\n",
       " 84: 'city',\n",
       " 85: 'work',\n",
       " 86: 'going',\n",
       " 87: 'before',\n",
       " 88: 'feel',\n",
       " 89: 'country',\n",
       " 90: 'anything',\n",
       " 91: 'live',\n",
       " 92: 'see',\n",
       " 93: 'you',\n",
       " 94: 'another',\n",
       " 95: 'all',\n",
       " 96: 't',\n",
       " 97: 'school',\n",
       " 98: 'place',\n",
       " 99: 'made',\n",
       " 100: 'come',\n",
       " 101: 'used',\n",
       " 102: 'more',\n",
       " 103: 'last',\n",
       " 104: 'part',\n",
       " 105: 'know',\n",
       " 106: 'could',\n",
       " 107: 'something',\n",
       " 108: 'someone',\n",
       " 109: 'use',\n",
       " 110: 'most',\n",
       " 111: 'its',\n",
       " 112: 'play',\n",
       " 113: 'help',\n",
       " 114: 'type',\n",
       " 115: 'whom',\n",
       " 116: 'good',\n",
       " 117: 'start',\n",
       " 118: 'state',\n",
       " 119: 'being',\n",
       " 120: 'new',\n",
       " 121: 'known',\n",
       " 122: 'find',\n",
       " 123: 'man',\n",
       " 124: 'now',\n",
       " 125: 'story',\n",
       " 126: 'job',\n",
       " 127: 'next',\n",
       " 128: 'found',\n",
       " 129: 'person',\n",
       " 130: 'located',\n",
       " 131: 'home',\n",
       " 132: 'ever',\n",
       " 133: 'happen',\n",
       " 134: 'show',\n",
       " 135: 'two',\n",
       " 136: 'than',\n",
       " 137: 'into',\n",
       " 138: 'give',\n",
       " 139: 'some',\n",
       " 140: 'win',\n",
       " 141: 'tell',\n",
       " 142: 'back',\n",
       " 143: 'article',\n",
       " 144: 'group',\n",
       " 145: 'so',\n",
       " 146: 'if',\n",
       " 147: 'happy',\n",
       " 148: 'become',\n",
       " 149: 'money',\n",
       " 150: 'area',\n",
       " 151: 'die',\n",
       " 152: 'house',\n",
       " 153: 'things',\n",
       " 154: 'father',\n",
       " 155: 'over',\n",
       " 156: 'during',\n",
       " 157: 'language',\n",
       " 158: 'whose',\n",
       " 159: 'these',\n",
       " 160: 'look',\n",
       " 161: 'color',\n",
       " 162: 'well',\n",
       " 163: 'still',\n",
       " 164: 'call',\n",
       " 165: 'said',\n",
       " 166: 'title',\n",
       " 167: 'mentioned',\n",
       " 168: 'ask',\n",
       " 169: 'put',\n",
       " 170: 'end',\n",
       " 171: 'years',\n",
       " 172: 'same',\n",
       " 173: 'born',\n",
       " 174: 'family',\n",
       " 175: 'leave',\n",
       " 176: 'won',\n",
       " 177: 'kids',\n",
       " 178: 'should',\n",
       " 179: 'talk',\n",
       " 180: 'lot',\n",
       " 181: 'got',\n",
       " 182: 'times',\n",
       " 183: 'away',\n",
       " 184: 'also',\n",
       " 185: 'way',\n",
       " 186: 'book',\n",
       " 187: 'world',\n",
       " 188: 'only',\n",
       " 189: 'population',\n",
       " 190: 'others',\n",
       " 191: 'friends',\n",
       " 192: 'names',\n",
       " 193: 'us',\n",
       " 194: 'party',\n",
       " 195: 'different',\n",
       " 196: 'came',\n",
       " 197: 'tom',\n",
       " 198: 'dad',\n",
       " 199: 'company',\n",
       " 200: 'died',\n",
       " 201: 'mean',\n",
       " 202: 'mother',\n",
       " 203: 'big',\n",
       " 204: 'main',\n",
       " 205: 'need',\n",
       " 206: 'far',\n",
       " 207: 'done',\n",
       " 208: 'named',\n",
       " 209: 'try',\n",
       " 210: 'countries',\n",
       " 211: 'capital',\n",
       " 212: 'around',\n",
       " 213: 'those',\n",
       " 214: 'went',\n",
       " 215: 'meet',\n",
       " 216: 'write',\n",
       " 217: 'president',\n",
       " 218: 'against',\n",
       " 219: 'eat',\n",
       " 220: 'men',\n",
       " 221: 'thing',\n",
       " 222: 'team',\n",
       " 223: 'war',\n",
       " 224: 'speak',\n",
       " 225: 'famous',\n",
       " 226: 'talking',\n",
       " 227: 'down',\n",
       " 228: 'left',\n",
       " 229: 'stay',\n",
       " 230: 'animal',\n",
       " 231: 'friend',\n",
       " 232: 'movie',\n",
       " 233: 'mom',\n",
       " 234: 'position',\n",
       " 235: 'according',\n",
       " 236: 'run',\n",
       " 237: 'children',\n",
       " 238: 'stand',\n",
       " 239: 'we',\n",
       " 240: 'often',\n",
       " 241: 'believe',\n",
       " 242: 'news',\n",
       " 243: 'largest',\n",
       " 244: 'might',\n",
       " 245: 'd',\n",
       " 246: 'second',\n",
       " 247: 'looking',\n",
       " 248: 'change',\n",
       " 249: 'john',\n",
       " 250: 'event',\n",
       " 251: 'founded',\n",
       " 252: 'room',\n",
       " 253: 'age',\n",
       " 254: 'game',\n",
       " 255: 'wanted',\n",
       " 256: 'too',\n",
       " 257: 'mr',\n",
       " 258: 'term',\n",
       " 259: 'agree',\n",
       " 260: 'important',\n",
       " 261: 'off',\n",
       " 262: 'living',\n",
       " 263: 'together',\n",
       " 264: 'trying',\n",
       " 265: 'created',\n",
       " 266: 'didn',\n",
       " 267: 'considered',\n",
       " 268: 'death',\n",
       " 269: 'night',\n",
       " 270: 'since',\n",
       " 271: 'popular',\n",
       " 272: 'stop',\n",
       " 273: 'rank',\n",
       " 274: 'able',\n",
       " 275: 'killed',\n",
       " 276: 'begin',\n",
       " 277: 'while',\n",
       " 278: 'word',\n",
       " 279: 'held',\n",
       " 280: 'life',\n",
       " 281: 'water',\n",
       " 282: 'married',\n",
       " 283: 'thought',\n",
       " 284: 'true',\n",
       " 285: 'took',\n",
       " 286: 'alone',\n",
       " 287: 'each',\n",
       " 288: 'own',\n",
       " 289: 'learn',\n",
       " 290: 'best',\n",
       " 291: 'author',\n",
       " 292: 'just',\n",
       " 293: 'boy',\n",
       " 294: 'dog',\n",
       " 295: 'keep',\n",
       " 296: 'started',\n",
       " 297: 'buy',\n",
       " 298: 'study',\n",
       " 299: 'always',\n",
       " 300: 'parents',\n",
       " 301: 'number',\n",
       " 302: 'wrote',\n",
       " 303: 'town',\n",
       " 304: 'month',\n",
       " 305: 'playing',\n",
       " 306: 'boys',\n",
       " 307: 'related',\n",
       " 308: 'everyone',\n",
       " 309: 'jack',\n",
       " 310: 'leader',\n",
       " 311: 'asked',\n",
       " 312: 'himself',\n",
       " 313: 'open',\n",
       " 314: 'visit',\n",
       " 315: 'students',\n",
       " 316: 'example',\n",
       " 317: 'under',\n",
       " 318: 'told',\n",
       " 319: 'seen',\n",
       " 320: 'states',\n",
       " 321: 'child',\n",
       " 322: 'involved',\n",
       " 323: 'chapter',\n",
       " 324: 'profession',\n",
       " 325: 'between',\n",
       " 326: 'organization',\n",
       " 327: 'bring',\n",
       " 328: 'brother',\n",
       " 329: 'gave',\n",
       " 330: 'instead',\n",
       " 331: 'week',\n",
       " 332: 'date',\n",
       " 333: 'move',\n",
       " 334: 'north',\n",
       " 335: 'woman',\n",
       " 336: 'food',\n",
       " 337: 'high',\n",
       " 338: 'working',\n",
       " 339: 'class',\n",
       " 340: 'character',\n",
       " 341: 'large',\n",
       " 342: 'sam',\n",
       " 343: 'near',\n",
       " 344: 'son',\n",
       " 345: 'body',\n",
       " 346: 'car',\n",
       " 347: 'given',\n",
       " 348: 'arrested',\n",
       " 349: 'cause',\n",
       " 350: 'reason',\n",
       " 351: 'nickname',\n",
       " 352: 'score',\n",
       " 353: 'better',\n",
       " 354: 'occur',\n",
       " 355: 'plan',\n",
       " 356: 'letter',\n",
       " 357: 'official',\n",
       " 358: 'getting',\n",
       " 359: 'government',\n",
       " 360: 'bad',\n",
       " 361: 'set',\n",
       " 362: 'young',\n",
       " 363: 'based',\n",
       " 364: 'land',\n",
       " 365: 'music',\n",
       " 366: 'close',\n",
       " 367: 'ones',\n",
       " 368: 'occupation',\n",
       " 369: 'travel',\n",
       " 370: 'english',\n",
       " 371: 'taken',\n",
       " 372: 'describe',\n",
       " 373: 'played',\n",
       " 374: 'dick',\n",
       " 375: 'languages',\n",
       " 376: 'season',\n",
       " 377: 'hear',\n",
       " 378: 'saw',\n",
       " 379: 'charge',\n",
       " 380: 'love',\n",
       " 381: 'right',\n",
       " 382: 'enjoy',\n",
       " 383: 'university',\n",
       " 384: 'today',\n",
       " 385: 'wife',\n",
       " 386: 'through',\n",
       " 387: 'small',\n",
       " 388: 'trip',\n",
       " 389: 'race',\n",
       " 390: 'caused',\n",
       " 391: 'hold',\n",
       " 392: 'system',\n",
       " 393: 'easy',\n",
       " 394: 'obama',\n",
       " 395: 'th',\n",
       " 396: 'helped',\n",
       " 397: 'wear',\n",
       " 398: 'having',\n",
       " 399: 'decide',\n",
       " 400: 'wearing',\n",
       " 401: 'mary',\n",
       " 402: 'false',\n",
       " 403: 'written',\n",
       " 404: 'idea',\n",
       " 405: 'mrs',\n",
       " 406: 'face',\n",
       " 407: 'ago',\n",
       " 408: 'published',\n",
       " 409: 'american',\n",
       " 410: 'lead',\n",
       " 411: 'accused',\n",
       " 412: 'special',\n",
       " 413: 'send',\n",
       " 414: 'head',\n",
       " 415: 'girls',\n",
       " 416: 'described',\n",
       " 417: 'coming',\n",
       " 418: 'i',\n",
       " 419: 'hit',\n",
       " 420: 'beat',\n",
       " 421: 'sport',\n",
       " 422: 'receive',\n",
       " 423: 'island',\n",
       " 424: 'court',\n",
       " 425: 'sent',\n",
       " 426: 'common',\n",
       " 427: 'teacher',\n",
       " 428: 'south',\n",
       " 429: 'lady',\n",
       " 430: 'offer',\n",
       " 431: 'released',\n",
       " 432: 'program',\n",
       " 433: 'lost',\n",
       " 434: 'books',\n",
       " 435: 'major',\n",
       " 436: 'current',\n",
       " 437: 'again',\n",
       " 438: 'problem',\n",
       " 439: 'spend',\n",
       " 440: 'period',\n",
       " 441: 'days',\n",
       " 442: 'record',\n",
       " 443: 'u',\n",
       " 444: 'return',\n",
       " 445: 'charged',\n",
       " 446: 'lose',\n",
       " 447: 'sitting',\n",
       " 448: 'teach',\n",
       " 449: 'top',\n",
       " 450: 'role',\n",
       " 451: 'case',\n",
       " 452: 'fight',\n",
       " 453: 'games',\n",
       " 454: 'nationality',\n",
       " 455: 'early',\n",
       " 456: 'hurt',\n",
       " 457: 'girl',\n",
       " 458: 'business',\n",
       " 459: 'share',\n",
       " 460: 'gone',\n",
       " 461: 'usually',\n",
       " 462: 'attend',\n",
       " 463: 'police',\n",
       " 464: 'information',\n",
       " 465: 'outside',\n",
       " 466: 'read',\n",
       " 467: 'great',\n",
       " 468: 'arrive',\n",
       " 469: 'college',\n",
       " 470: 'create',\n",
       " 471: 'alive',\n",
       " 472: 'china',\n",
       " 473: 'weather',\n",
       " 474: 'types',\n",
       " 475: 'region',\n",
       " 476: 'include',\n",
       " 477: 'sister',\n",
       " 478: 'walk',\n",
       " 479: 'office',\n",
       " 480: 'king',\n",
       " 481: 'crime',\n",
       " 482: 'pay',\n",
       " 483: 'doctor',\n",
       " 484: 'peter',\n",
       " 485: 'animals',\n",
       " 486: 'later',\n",
       " 487: 'places',\n",
       " 488: 'running',\n",
       " 489: 'making',\n",
       " 490: 'arrived',\n",
       " 491: 'very',\n",
       " 492: 'film',\n",
       " 493: 'injured',\n",
       " 494: 'form',\n",
       " 495: 'door',\n",
       " 496: 'older',\n",
       " 497: 'match',\n",
       " 498: 'free',\n",
       " 499: 'paper',\n",
       " 500: 'control',\n",
       " 501: 'border',\n",
       " 502: 'short',\n",
       " 503: 'total',\n",
       " 504: 'religion',\n",
       " 505: 'public',\n",
       " 506: 'let',\n",
       " 507: 'behind',\n",
       " 508: 'paul',\n",
       " 509: 'women',\n",
       " 510: 'store',\n",
       " 511: 'side',\n",
       " 512: 'west',\n",
       " 513: 'successful',\n",
       " 514: 'sit',\n",
       " 515: 'shot',\n",
       " 516: 'cost',\n",
       " 517: 'taking',\n",
       " 518: 'fall',\n",
       " 519: 'river',\n",
       " 520: 'bill',\n",
       " 521: 'act',\n",
       " 522: 'three',\n",
       " 523: 'support',\n",
       " 524: 'care',\n",
       " 525: 'real',\n",
       " 526: 'areas',\n",
       " 527: 'both',\n",
       " 528: 'sleep',\n",
       " 529: 'because',\n",
       " 530: 'power',\n",
       " 531: 'third',\n",
       " 532: 'wrong',\n",
       " 533: 'bob',\n",
       " 534: 'passed',\n",
       " 535: 'point',\n",
       " 536: 'happening',\n",
       " 537: 'but',\n",
       " 538: 'missing',\n",
       " 539: 'pass',\n",
       " 540: 'join',\n",
       " 541: 'favorite',\n",
       " 542: 'george',\n",
       " 543: 'county',\n",
       " 544: 'size',\n",
       " 545: 'original',\n",
       " 546: 'upset',\n",
       " 547: 'husband',\n",
       " 548: 'morning',\n",
       " 549: 'worried',\n",
       " 550: 'supposed',\n",
       " 551: 'james',\n",
       " 552: 'subject',\n",
       " 553: 'until',\n",
       " 554: 'recently',\n",
       " 555: 'lived',\n",
       " 556: 'century',\n",
       " 557: 'military',\n",
       " 558: 'guy',\n",
       " 559: 'modern',\n",
       " 560: 'became',\n",
       " 561: 'speaking',\n",
       " 562: 'frank',\n",
       " 563: 'along',\n",
       " 564: 'cities',\n",
       " 565: 'seem',\n",
       " 566: 'happens',\n",
       " 567: 'french',\n",
       " 568: 'tv',\n",
       " 569: 'rule',\n",
       " 570: 'full',\n",
       " 571: 'player',\n",
       " 572: 'daughter',\n",
       " 573: 'member',\n",
       " 574: 'less',\n",
       " 575: 'boat',\n",
       " 576: 'purpose',\n",
       " 577: 'problems',\n",
       " 578: 'groups',\n",
       " 579: 'building',\n",
       " 580: 'serve',\n",
       " 581: 'originally',\n",
       " 582: 'hospital',\n",
       " 583: 'sick',\n",
       " 584: 'final',\n",
       " 585: 'writing',\n",
       " 586: 'walking',\n",
       " 587: 'received',\n",
       " 588: 'needed',\n",
       " 589: 'relationship',\n",
       " 590: 'law',\n",
       " 591: 'sold',\n",
       " 592: 'holding',\n",
       " 593: 'islands',\n",
       " 594: 'joe',\n",
       " 595: 'heard',\n",
       " 596: 'waiting',\n",
       " 597: 'meal',\n",
       " 598: 'sell',\n",
       " 599: 'tried',\n",
       " 600: 'once',\n",
       " 601: 'song',\n",
       " 602: 'changed',\n",
       " 603: 'hand',\n",
       " 604: 'goal',\n",
       " 605: 'allowed',\n",
       " 606: 'attack',\n",
       " 607: 'political',\n",
       " 608: 'air',\n",
       " 609: 'anywhere',\n",
       " 610: 'grow',\n",
       " 611: 'location',\n",
       " 612: 'east',\n",
       " 613: 'issue',\n",
       " 614: 'baby',\n",
       " 615: 'claim',\n",
       " 616: 'united',\n",
       " 617: 'eyes',\n",
       " 618: 'using',\n",
       " 619: 'america',\n",
       " 620: 'every',\n",
       " 621: 'phone',\n",
       " 622: 'hope',\n",
       " 623: 'members',\n",
       " 624: 'little',\n",
       " 625: 'planning',\n",
       " 626: 'award',\n",
       " 627: 'besides',\n",
       " 628: 'trial',\n",
       " 629: 'words',\n",
       " 630: 'question',\n",
       " 631: 'met',\n",
       " 632: 'dinner',\n",
       " 633: 'harry',\n",
       " 634: 'london',\n",
       " 635: 'chinese',\n",
       " 636: 'report',\n",
       " 637: 'spoke',\n",
       " 638: 'included',\n",
       " 639: 'wasn',\n",
       " 640: 'meeting',\n",
       " 641: 'makes',\n",
       " 642: 'eating',\n",
       " 643: 'answer',\n",
       " 644: 'turn',\n",
       " 645: 'hair',\n",
       " 646: 'oldest',\n",
       " 647: 'trouble',\n",
       " 648: 'late',\n",
       " 649: 'miss',\n",
       " 650: 'watch',\n",
       " 651: 'dave',\n",
       " 652: 'brought',\n",
       " 653: 'says',\n",
       " 654: 'ride',\n",
       " 655: 'art',\n",
       " 656: 'order',\n",
       " 657: 'appear',\n",
       " 658: 'guilty',\n",
       " 659: 'available',\n",
       " 660: 'similar',\n",
       " 661: 'white',\n",
       " 662: 'hard',\n",
       " 663: 'sound',\n",
       " 664: 'nice',\n",
       " 665: 'jim',\n",
       " 666: 'rest',\n",
       " 667: 'expected',\n",
       " 668: 'incident',\n",
       " 669: 'biggest',\n",
       " 670: 'research',\n",
       " 671: 'sort',\n",
       " 672: 'empire',\n",
       " 673: 'feeling',\n",
       " 674: 'narrator',\n",
       " 675: 'such',\n",
       " 676: 'afraid',\n",
       " 677: 'react',\n",
       " 678: 'follow',\n",
       " 679: 'siblings',\n",
       " 680: 'really',\n",
       " 681: 'british',\n",
       " 682: 'works',\n",
       " 683: 'park',\n",
       " 684: 'history',\n",
       " 685: 'social',\n",
       " 686: 'specific',\n",
       " 687: 'sad',\n",
       " 688: 'kill',\n",
       " 689: 'jail',\n",
       " 690: 'characters',\n",
       " 691: 'band',\n",
       " 692: 'compared',\n",
       " 693: 'doesn',\n",
       " 694: 'drink',\n",
       " 695: 'suggest',\n",
       " 696: 'focus',\n",
       " 697: 'may',\n",
       " 698: 'light',\n",
       " 699: 'star',\n",
       " 700: 'europe',\n",
       " 701: 'result',\n",
       " 702: 'mark',\n",
       " 703: 'student',\n",
       " 704: 'compare',\n",
       " 705: 'decision',\n",
       " 706: 'site',\n",
       " 707: 'no',\n",
       " 708: 'wait',\n",
       " 709: 'previous',\n",
       " 710: 'france',\n",
       " 711: 'project',\n",
       " 712: 'talked',\n",
       " 713: 'album',\n",
       " 714: 'reported',\n",
       " 715: 'competition',\n",
       " 716: 'train',\n",
       " 717: 'perform',\n",
       " 718: 'present',\n",
       " 719: 'charges',\n",
       " 720: 'germany',\n",
       " 721: 'thinks',\n",
       " 722: 'belong',\n",
       " 723: 'terms',\n",
       " 724: 'table',\n",
       " 725: 'even',\n",
       " 726: 'ranked',\n",
       " 727: 'attorney',\n",
       " 728: 'reach',\n",
       " 729: 'list',\n",
       " 730: 'message',\n",
       " 731: 'couldn',\n",
       " 732: 'tree',\n",
       " 733: 'election',\n",
       " 734: 'church',\n",
       " 735: 'media',\n",
       " 736: 'topic',\n",
       " 737: 'nation',\n",
       " 738: 'kid',\n",
       " 739: 'rich',\n",
       " 740: 'cnn',\n",
       " 741: 'leaving',\n",
       " 742: 'headed',\n",
       " 743: 'cover',\n",
       " 744: 'watching',\n",
       " 745: 'ready',\n",
       " 746: 'excited',\n",
       " 747: 'already',\n",
       " 748: 'network',\n",
       " 749: 'ocean',\n",
       " 750: 'companies',\n",
       " 751: 'horse',\n",
       " 752: 'borders',\n",
       " 753: 'themselves',\n",
       " 754: 'kinds',\n",
       " 755: 'percentage',\n",
       " 756: 'former',\n",
       " 757: 'parts',\n",
       " 758: 'sign',\n",
       " 759: 'sea',\n",
       " 760: 'lives',\n",
       " 761: 'fish',\n",
       " 762: 'india',\n",
       " 763: 'beginning',\n",
       " 764: 'pet',\n",
       " 765: 'service',\n",
       " 766: 'schools',\n",
       " 767: 'currently',\n",
       " 768: 'amount',\n",
       " 769: 'video',\n",
       " 770: 'looked',\n",
       " 771: 'station',\n",
       " 772: 'holiday',\n",
       " 773: 'caught',\n",
       " 774: 'field',\n",
       " 775: 'provide',\n",
       " 776: 'national',\n",
       " 777: 'hands',\n",
       " 778: 'past',\n",
       " 779: 'secret',\n",
       " 780: 'led',\n",
       " 781: 'herself',\n",
       " 782: 'save',\n",
       " 783: 'situation',\n",
       " 784: 'felt',\n",
       " 785: 'responsible',\n",
       " 786: 'lawyer',\n",
       " 787: 'younger',\n",
       " 788: 'movies',\n",
       " 789: 'david',\n",
       " 790: 'fun',\n",
       " 791: 'grade',\n",
       " 792: 'anne',\n",
       " 793: 'sun',\n",
       " 794: 'stories',\n",
       " 795: 'male',\n",
       " 796: 'established',\n",
       " 797: 'sometimes',\n",
       " 798: 'ben',\n",
       " 799: 'finally',\n",
       " 800: 'tournament',\n",
       " 801: 'scared',\n",
       " 802: 'michael',\n",
       " 803: 'liked',\n",
       " 804: 'actually',\n",
       " 805: 'ship',\n",
       " 806: 'reading',\n",
       " 807: 'helping',\n",
       " 808: 'build',\n",
       " 809: 'cold',\n",
       " 810: 'center',\n",
       " 811: 'european',\n",
       " 812: 'captain',\n",
       " 813: 'represent',\n",
       " 814: 'discovered',\n",
       " 815: 'culture',\n",
       " 816: 'line',\n",
       " 817: 'wish',\n",
       " 818: 'street',\n",
       " 819: 'soon',\n",
       " 820: 'notice',\n",
       " 821: 'owns',\n",
       " 822: 'wake',\n",
       " 823: 'exactly',\n",
       " 824: 'accident',\n",
       " 825: 'brothers',\n",
       " 826: 'consider',\n",
       " 827: 'union',\n",
       " 828: 'announced',\n",
       " 829: 'space',\n",
       " 830: 'search',\n",
       " 831: 'jane',\n",
       " 832: 'refer',\n",
       " 833: 'fast',\n",
       " 834: 'marry',\n",
       " 835: 'invited',\n",
       " 836: 'increase',\n",
       " 837: 'middle',\n",
       " 838: 'suspect',\n",
       " 839: 'standing',\n",
       " 840: 'choose',\n",
       " 841: 'teams',\n",
       " 842: 'built',\n",
       " 843: 'theory',\n",
       " 844: 'breakfast',\n",
       " 845: 'worked',\n",
       " 846: 'jobs',\n",
       " 847: 'break',\n",
       " 848: 'poor',\n",
       " 849: 'interview',\n",
       " 850: 'eventually',\n",
       " 851: 'single',\n",
       " 852: 'bought',\n",
       " 853: 'particular',\n",
       " 854: 'percent',\n",
       " 855: 'female',\n",
       " 856: 'test',\n",
       " 857: 'clothes',\n",
       " 858: 'judge',\n",
       " 859: 'victim',\n",
       " 860: 'surprised',\n",
       " 861: 'earlier',\n",
       " 862: 'decided',\n",
       " 863: 'awards',\n",
       " 864: 'concerned',\n",
       " 865: 'henry',\n",
       " 866: 'cat',\n",
       " 867: 'moved',\n",
       " 868: 'ended',\n",
       " 869: 'fred',\n",
       " 870: 'england',\n",
       " 871: 'larger',\n",
       " 872: 'everything',\n",
       " 873: 'enough',\n",
       " 874: 'associated',\n",
       " 875: 'front',\n",
       " 876: 'whole',\n",
       " 877: 'spoken',\n",
       " 878: 'letters',\n",
       " 879: 'continue',\n",
       " 880: 'birthday',\n",
       " 881: 'black',\n",
       " 882: 'pick',\n",
       " 883: 'inside',\n",
       " 884: 'safe',\n",
       " 885: 'worth',\n",
       " 886: 'response',\n",
       " 887: 'style',\n",
       " 888: 'thinking',\n",
       " 889: 'tall',\n",
       " 890: 'deal',\n",
       " 891: 'ways',\n",
       " 892: 'steve',\n",
       " 893: 'agency',\n",
       " 894: 'vehicle',\n",
       " 895: 'convicted',\n",
       " 896: 'speech',\n",
       " 897: 'quickly',\n",
       " 898: 'ranking',\n",
       " 899: 'sentence',\n",
       " 900: 'possible',\n",
       " 901: 'league',\n",
       " 902: 'branch',\n",
       " 903: 'sports',\n",
       " 904: 'force',\n",
       " 905: 'release',\n",
       " 906: 'don',\n",
       " 907: 'dream',\n",
       " 908: 'expect',\n",
       " 909: 'meaning',\n",
       " 910: 'uses',\n",
       " 911: 'driving',\n",
       " 912: 'prison',\n",
       " 913: 'international',\n",
       " 914: 'here',\n",
       " 915: 'me',\n",
       " 916: 'few',\n",
       " 917: 'angry',\n",
       " 918: 'events',\n",
       " 919: 'finish',\n",
       " 920: 'career',\n",
       " 921: 'likely',\n",
       " 922: 'york',\n",
       " 923: 'education',\n",
       " 924: 'version',\n",
       " 925: 'general',\n",
       " 926: 'murder',\n",
       " 927: 'battle',\n",
       " 928: 'australia',\n",
       " 929: 'thomas',\n",
       " 930: 'means',\n",
       " 931: 'brown',\n",
       " 932: 'drive',\n",
       " 933: 'picture',\n",
       " 934: 'catch',\n",
       " 935: 'recent',\n",
       " 936: 'yet',\n",
       " 937: 'relation',\n",
       " 938: 'fighting',\n",
       " 939: 'director',\n",
       " 940: 'understand',\n",
       " 941: 'spanish',\n",
       " 942: 'stopped',\n",
       " 943: 'religious',\n",
       " 944: 'earth',\n",
       " 945: 'host',\n",
       " 946: 'allow',\n",
       " 947: 'initially',\n",
       " 948: 'mostly',\n",
       " 949: 'german',\n",
       " 950: 'owned',\n",
       " 951: 'paid',\n",
       " 952: 'listed',\n",
       " 953: 'wants',\n",
       " 954: 'mention',\n",
       " 955: 'prize',\n",
       " 956: 'britain',\n",
       " 957: 'cup',\n",
       " 958: 'province',\n",
       " 959: 'magazine',\n",
       " 960: 'club',\n",
       " 961: 'mike',\n",
       " 962: 'giving',\n",
       " 963: 'developed',\n",
       " 964: 'across',\n",
       " 965: 'please',\n",
       " 966: 'human',\n",
       " 967: 'opened',\n",
       " 968: 'visited',\n",
       " 969: 'normal',\n",
       " 970: 'territory',\n",
       " 971: 'interested',\n",
       " 972: 'experience',\n",
       " 973: 'stuff',\n",
       " 974: 'post',\n",
       " 975: 'billy',\n",
       " 976: 'fire',\n",
       " 977: 'bird',\n",
       " 978: 'martin',\n",
       " 979: 'hearing',\n",
       " 980: 'gain',\n",
       " 981: 'ruled',\n",
       " 982: 'carry',\n",
       " 983: 'without',\n",
       " 984: 'weapon',\n",
       " 985: 'jimmy',\n",
       " 986: 'publish',\n",
       " 987: 'formed',\n",
       " 988: 'sure',\n",
       " 989: 'half',\n",
       " 990: 'hide',\n",
       " 991: 'kingdom',\n",
       " 992: 'woods',\n",
       " 993: 'bigger',\n",
       " 994: 'bed',\n",
       " 995: 'olympics',\n",
       " 996: 'evidence',\n",
       " 997: 'knew',\n",
       " 998: 'must',\n",
       " 999: 'remain',\n",
       " 1000: 'driver',\n",
       " ...}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing tokenizer indices of the words \n",
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################                Training                ####################################\n",
    "## thi part was ran at colab\n",
    "\n",
    "# Defining LSTM model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(NUM_WORDS, 100, input_length=MAX_LEN))\n",
    "model.add(LSTM(LSTM_EMBED, dropout=0.2, recurrent_dropout=0.2, input_shape=(train_data.shape[1], NUM_WORDS)))\n",
    "model.add(RepeatVector(train_data.shape[-1]))\n",
    "model.add(LSTM(LSTM_EMBED, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(Dense(NUM_WORDS, activation='softmax'))\n",
    "# Model compiling\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the checkpoint (to save the model after each epoch)\n",
    "filepath = \"lstm-encoder-{epoch:02d}-{loss:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fitting the model (training for 25 epoch with 32 batch size)\n",
    "model.fit(train_data, np.expand_dims(train_data, -1), epochs=25, batch_size= BATCH_SIZE, callbacks=callbacks_list)\n",
    "# Saving the final model\n",
    "model.save(\"lstm-encoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108616/108616 [==============================] - 730s 7ms/sample - loss: 1.1700 - acc: 0.8130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1700287287651618, 0.81302434]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the loss and he accuracy of the final saved model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.python.keras.models import load_model\n",
    "loaded_model = load_model(\"lstm-encoder.h5\")\n",
    "loaded_model.evaluate(train_data, np.expand_dims(train_data, -1), batch_size= BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building the chatbot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0331 17:53:05.172277 140399519250176 deprecation.py:506] From /home/zeinab/miniconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0331 17:53:05.274111 140399519250176 deprecation.py:506] From /home/zeinab/miniconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0331 17:53:07.758755 140399519250176 deprecation.py:323] From /home/zeinab/miniconda3/envs/myenv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from six.moves import cPickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "\n",
    "# Defining the Global variables \n",
    "BATCH_SIZE = 32 # Batch size for GPU\n",
    "NUM_WORDS = 10000 # Vocab length\n",
    "MAX_LEN = 20 # Padding length (# of words)\n",
    "LSTM_EMBED = 8 # Number of LSTM nodes\n",
    "K.set_learning_phase(False)\n",
    "\n",
    "# Reading the dataset\n",
    "dataset = pd.read_csv(\"Rearranged_data.csv\")\n",
    "# Removing context number column\n",
    "data = dataset[['Questions','Answers']]\n",
    "dataset = []\n",
    "# Cleaning the questions\n",
    "data.Questions = data.Questions.apply(lambda x: (re.sub('[^a-zA-Z]', ' ', x)).strip())\n",
    "data.Questions = data.Questions.apply(lambda x: ' '.join(x.lower().split()))\n",
    "\n",
    "# Loading the tokenizer\n",
    "tokenizer = cPickle.load(open(\"lstm-autoencoder-tokenizer.pickle\", \"rb\"))\n",
    "# Questions tokenization\n",
    "Questions = tokenizer.texts_to_sequences(data.Questions)\n",
    "# Padding sequences that are shorter than MAX_LEN\n",
    "Questions = pad_sequences(Questions, padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "\n",
    "# Reading the encoder model\n",
    "model = load_model(\"lstm-encoder.h5\")\n",
    "# Creating the encoding function\n",
    "encode = K.function([model.input], [model.layers[1].output])\n",
    "# Encoding the questions with the predefined function\n",
    "Questions = np.squeeze(np.array(encode([Questions])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now I will try the same conversations that i tried using TF-IDF chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First conversation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to CoQa chatbot\n",
      "Please enter a question:  When was the Vat formally opened?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" It was formally established in 1475 \" with similarity:  1.0\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? Y\n",
      "Please enter another question: for what subjects?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" history, and law \" with similarity:  0.99999994\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? Y\n",
      "Please enter another question: what must be requested in person or by mail?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" Photocopies \" with similarity:  0.99999994\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? Y\n",
      "Please enter another question: when were the Secret Archives moved from the rest of the library?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" at the beginning of the 17th century; \" with similarity:  1.0\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? Y\n",
      "Please enter another question: what is the point of the project started in 2014?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" digitising manuscripts \" with similarity:  0.9999998\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? Y\n",
      "Please enter another question: what will this allow?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" them to be viewed online. \" with similarity:  1.0000001\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? N\n",
      "\n",
      "Good bye\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to CoQa chatbot\")\n",
    "question = input('Please enter a question: ')\n",
    "\n",
    "while True:\n",
    "    \"\"\"Cleaning input Question and Vectorizing it \"\"\"\n",
    "    # Cleaning \n",
    "    question = (re.sub('[^a-zA-Z]', ' ', question)).strip()\n",
    "    question = ' '.join(question.lower().split())\n",
    "    # Tokenization\n",
    "    question = tokenizer.texts_to_sequences([question])\n",
    "    # Padding to MAX_LEN\n",
    "    question = pad_sequences(question, padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "    # Encodding\n",
    "    question = np.squeeze(encode([question]))\n",
    "    \n",
    "    \"\"\"Cosine Simularity\"\"\"\n",
    "    rank = cosine_similarity(question.reshape(1, -1), Questions)\n",
    "    \n",
    "    \"\"\"Getting maximum cosine simularity rank\"\"\"\n",
    "    print(\"\\nSearching for the best answer..........\\n\")   \n",
    "    array=np.asarray(rank)\n",
    "    array=array.reshape(len(data),1)\n",
    "    maximum = max(array)\n",
    "    top_one =np.argsort(-1*rank, axis=-1).T[:1].tolist()\n",
    "    \n",
    "    \"\"\"Checking if maximum cosine simularity rank<0.5 \"\"\"\n",
    "    for item in top_one:\n",
    "        \n",
    "      if (maximum < 0.5):\n",
    "        print(\"Sorry, I can't find the answer\\n\")\n",
    "        \n",
    "        \"\"\" printting the answers of the top five quesions in cosine\n",
    "         simularity\"\"\"\n",
    "        print('Answers of the most similar five Questions\\n')\n",
    "        top_five = np.argsort(-1*rank, axis=-1).T[:5].tolist()\n",
    "        for item in top_five:\n",
    "          print(data['Questions'].iloc[item].values[0],' : ' , data['Answers'].iloc[item].values[0]) \n",
    "\n",
    "      else:\n",
    "        print('The answer is \"', \n",
    "              data['Answers'].iloc[item].values[0],'\" with similarity: ', maximum[0])  \n",
    "    print(\"\\n ---------------------------------------------------------\\n\")\n",
    "    \n",
    "    flag = True\n",
    "    flag_N= False\n",
    "    while flag:\n",
    "          do = [input('Do you have another question (Y/N)? ')]\n",
    "          if do[0] == 'Y':\n",
    "             question = input('Please enter another question: ')\n",
    "             flag = False\n",
    "          elif do[0] == 'N':\n",
    "                print('\\nGood bye')\n",
    "                flag = False\n",
    "                flag_N = True\n",
    "          else:\n",
    "             print(\"\\n I can't understand\\n\")\n",
    "\n",
    "    if flag_N == True:\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### we can notice that simularity is less than one in some questions unlike the TF-IDF method they were almost all having simularity equal to 1, i think this is logical because LSTM is compressing the vectors while keeping the important info. as much as possible so it may lose some info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to CoQa chatbot\n",
      "Please enter a question: Is the JPEG format supported by Adobe Flash Player 11.0?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" yes \" with similarity:  1.0000001\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? Y\n",
      "Please enter another question:  what is JPEG XR short for?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" JPEG extended range \" with similarity:  1.0\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? Y\n",
      "Please enter another question: when did microsoft put HD Photo up for consideration to be named JPEG XR?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" July 2007 \" with similarity:  1.0\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? Y\n",
      "Please enter another question: what did they rename it to?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" HD Photo \" with similarity:  1.0\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? N\n",
      "\n",
      "Good bye\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to CoQa chatbot\")\n",
    "question = input('Please enter a question: ')\n",
    "\n",
    "while True:\n",
    "    \"\"\"Cleaning input Question and Vectorizing it \"\"\"\n",
    "    # Cleaning \n",
    "    question = (re.sub('[^a-zA-Z]', ' ', question)).strip()\n",
    "    question = ' '.join(question.lower().split())\n",
    "    # Tokenization\n",
    "    question = tokenizer.texts_to_sequences([question])\n",
    "    # Padding to MAX_LEN\n",
    "    question = pad_sequences(question, padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "    # Encodding\n",
    "    question = np.squeeze(encode([question]))\n",
    "    \n",
    "    \"\"\"Cosine Simularity\"\"\"\n",
    "    rank = cosine_similarity(question.reshape(1, -1), Questions)\n",
    "    \n",
    "    \"\"\"Getting maximum cosine simularity rank\"\"\"\n",
    "    print(\"\\nSearching for the best answer..........\\n\")   \n",
    "    array=np.asarray(rank)\n",
    "    array=array.reshape(len(data),1)\n",
    "    maximum = max(array)\n",
    "    top_one =np.argsort(-1*rank, axis=-1).T[:1].tolist()\n",
    "    \n",
    "    \"\"\"Checking if maximum cosine simularity rank<0.5 \"\"\"\n",
    "    for item in top_one:\n",
    "        \n",
    "      if (maximum < 0.5):\n",
    "        print(\"Sorry, I can't find the answer\\n\")\n",
    "        \n",
    "        \"\"\" printting the answers of the top five quesions in cosine\n",
    "         simularity\"\"\"\n",
    "        print('Answers of the most similar five Questions\\n')\n",
    "        top_five = np.argsort(-1*rank, axis=-1).T[:5].tolist()\n",
    "        for item in top_five:\n",
    "          print(data['Questions'].iloc[item].values[0],' : ' , data['Answers'].iloc[item].values[0]) \n",
    "\n",
    "      else:\n",
    "        print('The answer is \"', \n",
    "              data['Answers'].iloc[item].values[0],'\" with similarity: ', maximum[0])  \n",
    "    print(\"\\n ---------------------------------------------------------\\n\")\n",
    "    \n",
    "    flag = True\n",
    "    flag_N= False\n",
    "    while flag:\n",
    "          do = [input('Do you have another question (Y/N)? ')]\n",
    "          if do[0] == 'Y':\n",
    "             question = input('Please enter another question: ')\n",
    "             flag = False\n",
    "          elif do[0] == 'N':\n",
    "                print('\\nGood bye')\n",
    "                flag = False\n",
    "                flag_N = True\n",
    "          else:\n",
    "             print(\"\\n I can't understand\\n\")\n",
    "\n",
    "    if flag_N == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to CoQa chatbot\n",
      "Please enter a question: what is the GoI?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" a ship to travel to South America \" with similarity:  1.0000001\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? Y\n",
      "Please enter another question: where did India come from?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" the Indus river \" with similarity:  1.0\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? Y\n",
      "Please enter another question: how many states are in India?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" 29 \" with similarity:  1.0\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? Y\n",
      "Please enter another question: how was it created?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" by the constitution of India \" with similarity:  0.99999994\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? N\n",
      "\n",
      "Good bye\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to CoQa chatbot\")\n",
    "question = input('Please enter a question: ')\n",
    "\n",
    "while True:\n",
    "    \"\"\"Cleaning input Question and Vectorizing it \"\"\"\n",
    "    # Cleaning \n",
    "    question = (re.sub('[^a-zA-Z]', ' ', question)).strip()\n",
    "    question = ' '.join(question.lower().split())\n",
    "    # Tokenization\n",
    "    question = tokenizer.texts_to_sequences([question])\n",
    "    # Padding to MAX_LEN\n",
    "    question = pad_sequences(question, padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "    # Encodding\n",
    "    question = np.squeeze(encode([question]))\n",
    "    \n",
    "    \"\"\"Cosine Simularity\"\"\"\n",
    "    rank = cosine_similarity(question.reshape(1, -1), Questions)\n",
    "    \n",
    "    \"\"\"Getting maximum cosine simularity rank\"\"\"\n",
    "    print(\"\\nSearching for the best answer..........\\n\")   \n",
    "    array=np.asarray(rank)\n",
    "    array=array.reshape(len(data),1)\n",
    "    maximum = max(array)\n",
    "    top_one =np.argsort(-1*rank, axis=-1).T[:1].tolist()\n",
    "    \n",
    "    \"\"\"Checking if maximum cosine simularity rank<0.5 \"\"\"\n",
    "    for item in top_one:\n",
    "        \n",
    "      if (maximum < 0.5):\n",
    "        print(\"Sorry, I can't find the answer\\n\")\n",
    "        \n",
    "        \"\"\" printting the answers of the top five quesions in cosine\n",
    "         simularity\"\"\"\n",
    "        print('Answers of the most similar five Questions\\n')\n",
    "        top_five = np.argsort(-1*rank, axis=-1).T[:5].tolist()\n",
    "        for item in top_five:\n",
    "          print(data['Questions'].iloc[item].values[0],' : ' , data['Answers'].iloc[item].values[0]) \n",
    "\n",
    "      else:\n",
    "        print('The answer is \"', \n",
    "              data['Answers'].iloc[item].values[0],'\" with similarity: ', maximum[0])  \n",
    "    print(\"\\n ---------------------------------------------------------\\n\")\n",
    "    \n",
    "    flag = True\n",
    "    flag_N= False\n",
    "    while flag:\n",
    "          do = [input('Do you have another question (Y/N)? ')]\n",
    "          if do[0] == 'Y':\n",
    "             question = input('Please enter another question: ')\n",
    "             flag = False\n",
    "          elif do[0] == 'N':\n",
    "                print('\\nGood bye')\n",
    "                flag = False\n",
    "                flag_N = True\n",
    "          else:\n",
    "             print(\"\\n I can't understand\\n\")\n",
    "\n",
    "    if flag_N == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can notice that the answer of first question is wrong while tf-idf was giving the right answer, the wrong answer is for the question ' What is the Beagle? '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A conversation illustrates some mistakes due to the repetition of the questions in different contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to CoQa chatbot\n",
      "Please enter a question: What is the largest island?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" Australia \" with similarity:  1.0000001\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? Y\n",
      "Please enter another question: Where is it?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" southern Atlantic Ocean. \" with similarity:  0.99999994\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? Y\n",
      "Please enter another question: How many people live there?\n",
      "\n",
      "Searching for the best answer..........\n",
      "\n",
      "The answer is \" 7.2 million \" with similarity:  1.0\n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "Do you have another question (Y/N)? N\n",
      "\n",
      "Good bye\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to CoQa chatbot\")\n",
    "question = input('Please enter a question: ')\n",
    "\n",
    "while True:\n",
    "    \"\"\"Cleaning input Question and Vectorizing it \"\"\"\n",
    "    # Cleaning \n",
    "    question = (re.sub('[^a-zA-Z]', ' ', question)).strip()\n",
    "    question = ' '.join(question.lower().split())\n",
    "    # Tokenization\n",
    "    question = tokenizer.texts_to_sequences([question])\n",
    "    # Padding to MAX_LEN\n",
    "    question = pad_sequences(question, padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "    # Encodding\n",
    "    question = np.squeeze(encode([question]))\n",
    "    \n",
    "    \"\"\"Cosine Simularity\"\"\"\n",
    "    rank = cosine_similarity(question.reshape(1, -1), Questions)\n",
    "    \n",
    "    \"\"\"Getting maximum cosine simularity rank\"\"\"\n",
    "    print(\"\\nSearching for the best answer..........\\n\")   \n",
    "    array=np.asarray(rank)\n",
    "    array=array.reshape(len(data),1)\n",
    "    maximum = max(array)\n",
    "    top_one =np.argsort(-1*rank, axis=-1).T[:1].tolist()\n",
    "    \n",
    "    \"\"\"Checking if maximum cosine simularity rank<0.5 \"\"\"\n",
    "    for item in top_one:\n",
    "        \n",
    "      if (maximum < 0.5):\n",
    "        print(\"Sorry, I can't find the answer\\n\")\n",
    "        \n",
    "        \"\"\" printting the answers of the top five quesions in cosine\n",
    "         simularity\"\"\"\n",
    "        print('Answers of the most similar five Questions\\n')\n",
    "        top_five = np.argsort(-1*rank, axis=-1).T[:5].tolist()\n",
    "        for item in top_five:\n",
    "          print(data['Questions'].iloc[item].values[0],' : ' , data['Answers'].iloc[item].values[0]) \n",
    "\n",
    "      else:\n",
    "        print('The answer is \"', \n",
    "              data['Answers'].iloc[item].values[0],'\" with similarity: ', maximum[0])  \n",
    "    print(\"\\n ---------------------------------------------------------\\n\")\n",
    "    \n",
    "    flag = True\n",
    "    flag_N= False\n",
    "    while flag:\n",
    "          do = [input('Do you have another question (Y/N)? ')]\n",
    "          if do[0] == 'Y':\n",
    "             question = input('Please enter another question: ')\n",
    "             flag = False\n",
    "          elif do[0] == 'N':\n",
    "                print('\\nGood bye')\n",
    "                flag = False\n",
    "                flag_N = True\n",
    "          else:\n",
    "             print(\"\\n I can't understand\\n\")\n",
    "\n",
    "    if flag_N == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the previous conversation the right answers were:\n",
    "1. ' Australia ' and the answer that printed is right but TF-IDF ws giving a wrong answer\n",
    "2. ' south of earth ' and the answer that printed was for another question in another context about 'South Georgia and the South Sandwich Islands' with also the question 'Where is it?'\n",
    "3. ' The population is nearly as large as Shanghai's ' and the answer that printed was for another question in another context  about 'Hong Kong' with also the question 'How many people live there?'\n",
    "\n",
    "##### Also LSTM has the problem when the question is repeated in many context, and I think also if we changed a little in the question to be diffrernt from the dataset it will produce another mistakes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
